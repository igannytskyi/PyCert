import os
import json
import requests
import threading
from requests.auth import HTTPBasicAuth

# Jira Configuration
JIRA_BASE_URL = "https://yourcompany.atlassian.net"
PROJECT_KEY = "YOUR_PROJECT_KEY"
USERNAME = "your-email@example.com"
PASSWORD = "your-password"  # Store securely

# Output Directories
OUTPUT_DIR = "exported_issues"
ATTACHMENTS_DIR = os.path.join(OUTPUT_DIR, "attachments")
CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, "export_progress.json")

# Ensure directories exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(ATTACHMENTS_DIR, exist_ok=True)

# Attachment folder management
MAX_FILES_PER_FOLDER = 500
attachment_folder_index = 1
current_attachment_folder = os.path.join(ATTACHMENTS_DIR, f"part_{attachment_folder_index}")
os.makedirs(current_attachment_folder, exist_ok=True)

# Pagination & Performance Settings
BATCH_SIZE = 500
MAX_RETRIES = 3

# Authenticate and Get Session Token
def get_jira_session():
    """Authenticate using username & password and return session headers."""
    auth = HTTPBasicAuth(USERNAME, PASSWORD)
    headers = {"Accept": "application/json"}
    return auth, headers

# Load authentication
AUTH, HEADERS = get_jira_session()

def fetch_issues(start_at):
    """Fetch issues in batches from Jira API v2."""
    url = f"{JIRA_BASE_URL}/rest/api/2/search"
    params = {
        "jql": f"project={PROJECT_KEY}",
        "startAt": start_at,
        "maxResults": BATCH_SIZE,
        "fields": "summary,description,attachment"
    }

    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = requests.get(url, auth=AUTH, headers=HEADERS, params=params)
            response.raise_for_status()
            return response.json().get("issues", [])
        except requests.RequestException as e:
            print(f"Retrying due to error: {e}")
            retries += 1
    return []

def fetch_comments(issue_key):
    """Fetch comments for an issue (Jira API v2) and check for attachments."""
    url = f"{JIRA_BASE_URL}/rest/api/2/issue/{issue_key}/comment"

    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = requests.get(url, auth=AUTH, headers=HEADERS)
            response.raise_for_status()
            comments = response.json().get("comments", [])

            formatted_comments = []
            for comment in comments:
                body = comment.get("body", "No Comment")
                author = comment.get("author", {}).get("displayName", "Unknown")
                attachments = []

                # Check for manually added attachment URLs in comment body
                if "[^" in body:  # Jira attachment format: [^filename.ext]
                    attachments = extract_attachments(body, issue_key)

                formatted_comments.append({
                    "author": author,
                    "body": body,
                    "attachments": attachments
                })
            return formatted_comments
        except requests.RequestException as e:
            print(f"Error fetching comments for {issue_key}: {e}")
            retries += 1
    return []

def extract_attachments(comment_body, issue_key):
    """Extract attachment file names from comment body and download them."""
    import re
    attachment_pattern = r"\[\^([^\]]+)\]"  # Matches [^filename.ext]
    matches = re.findall(attachment_pattern, comment_body)

    attachments = []
    for filename in matches:
        file_url = f"{JIRA_BASE_URL}/secure/attachment/{filename}"
        file_path = download_attachment(file_url, f"{issue_key}_comment_{filename}")
        if file_path:
            attachments.append({"filename": filename, "path": file_path})
    
    return attachments

def get_attachment_folder():
    """Ensures that attachments are stored in folders with max 500 files each."""
    global attachment_folder_index, current_attachment_folder

    # Count current files in folder
    current_file_count = len(os.listdir(current_attachment_folder))

    # If the current folder reaches the limit, create a new folder
    if current_file_count >= MAX_FILES_PER_FOLDER:
        attachment_folder_index += 1
        current_attachment_folder = os.path.join(ATTACHMENTS_DIR, f"part_{attachment_folder_index}")
        os.makedirs(current_attachment_folder, exist_ok=True)

    return current_attachment_folder

def download_attachment(url, filename):
    """Download an attachment from Jira and save it in a structured folder."""
    try:
        response = requests.get(url, auth=AUTH, headers=HEADERS, stream=True)
        response.raise_for_status()
        
        folder = get_attachment_folder()  # Get correct folder to store file
        filepath = os.path.join(folder, filename)

        with open(filepath, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        return filepath
    except requests.RequestException as e:
        print(f"Failed to download {filename}: {e}")
        return None

def process_issues():
    """Fetch, process, and store issues with comments and attachments."""
    last_index = 0  # Resume from here if needed
    part = 1

    while True:
        issues = fetch_issues(last_index)
        if not issues:
            print("No more issues to process.")
            break

        part_file = os.path.join(OUTPUT_DIR, f"issues_part_{part}.jsonl")
        with open(part_file, "a", encoding="utf-8") as f:
            for issue in issues:
                issue_key = issue["key"]
                summary = issue["fields"].get("summary", "No Summary")
                description = issue["fields"].get("description", "No Description")
                
                # Process issue attachments
                attachments = []
                for attachment in issue["fields"].get("attachment", []):
                    filename = f"{issue_key}_{attachment['filename']}"
                    file_path = download_attachment(attachment["content"], filename)
                    if file_path:
                        attachments.append({"filename": attachment["filename"], "path": file_path})

                # Fetch comments and check for attachments
                comments = fetch_comments(issue_key)

                # Save issue as a JSON line
                f.write(json.dumps({
                    "issue_key": issue_key,
                    "summary": summary,
                    "description": description,
                    "attachments": attachments,
                    "comments": comments
                }) + "\n")

            last_index += len(issues)

        # Rotate file every 50,000 issues
        if last_index % 50000 == 0:
            part += 1

    print(f"Export completed! Issues saved in {OUTPUT_DIR}")

if __name__ == "__main__":
    process_issues()

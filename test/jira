import os
import json
import requests
import threading
from requests.auth import HTTPBasicAuth
from queue import Queue

# Jira Configuration
JIRA_BASE_URL = "https://yourcompany.atlassian.net"
PROJECT_KEY = "YOUR_PROJECT_KEY"
USERNAME = "your-email@example.com"
PASSWORD = "your-password"  # Store securely

# Output Directories
OUTPUT_DIR = "exported_issues"
ATTACHMENTS_DIR = os.path.join(OUTPUT_DIR, "attachments")
CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, "export_progress.json")

# Ensure directories exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(ATTACHMENTS_DIR, exist_ok=True)

# Attachment folder management
MAX_FILES_PER_FOLDER = 500
attachment_folder_index = 1
current_attachment_folder = os.path.join(ATTACHMENTS_DIR, f"part_{attachment_folder_index}")
os.makedirs(current_attachment_folder, exist_ok=True)

# Pagination & Performance Settings
BATCH_SIZE = 500
MAX_RETRIES = 3
NUM_THREADS = 5  # Number of concurrent threads

# Authenticate and Get Session Token
def get_jira_session():
    """Authenticate using username & password and return session headers."""
    auth = HTTPBasicAuth(USERNAME, PASSWORD)
    headers = {"Accept": "application/json"}
    return auth, headers

# Load authentication
AUTH, HEADERS = get_jira_session()

def fetch_issues(start_at):
    """Fetch issues in batches from Jira API v2."""
    url = f"{JIRA_BASE_URL}/rest/api/2/search"
    params = {
        "jql": f"project={PROJECT_KEY}",
        "startAt": start_at,
        "maxResults": BATCH_SIZE,
        "fields": "summary,description,attachment"
    }

    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = requests.get(url, auth=AUTH, headers=HEADERS, params=params)
            response.raise_for_status()
            return response.json().get("issues", [])
        except requests.RequestException as e:
            print(f"Retrying due to error: {e}")
            retries += 1
    return []

def fetch_comments(issue_key):
    """Fetch comments for an issue (Jira API v2) and check for attachments."""
    url = f"{JIRA_BASE_URL}/rest/api/2/issue/{issue_key}/comment"

    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = requests.get(url, auth=AUTH, headers=HEADERS)
            response.raise_for_status()
            comments = response.json().get("comments", [])

            formatted_comments = []
            for comment in comments:
                body = comment.get("body", "No Comment")
                author = comment.get("author", {}).get("displayName", "Unknown")
                attachments = []

                # Check for manually added attachment URLs in comment body
                if "[^" in body:  # Jira attachment format: [^filename.ext]
                    attachments = extract_attachments(body, issue_key)

                formatted_comments.append({
                    "author": author,
                    "body": body,
                    "attachments": attachments
                })
            return formatted_comments
        except requests.RequestException as e:
            print(f"Error fetching comments for {issue_key}: {e}")
            retries += 1
    return []

def extract_attachments(comment_body, issue_key):
    """Extract attachment file names from comment body and download them."""
    import re
    attachment_pattern = r"\[\^([^\]]+)\]"  # Matches [^filename.ext]
    matches = re.findall(attachment_pattern, comment_body)

    attachments = []
    for filename in matches:
        file_url = f"{JIRA_BASE_URL}/secure/attachment/{filename}"
        file_path = download_attachment(issue_key, filename, f"{issue_key}_comment_{filename}")
        if file_path:
            attachments.append({"filename": filename, "path": file_path})
    
    return attachments

def get_attachment_folder():
    """Ensures that attachments are stored in folders with max 500 files each."""
    global attachment_folder_index, current_attachment_folder

    # Count current files in folder
    current_file_count = len(os.listdir(current_attachment_folder))

    # If the current folder reaches the limit, create a new folder
    if current_file_count >= MAX_FILES_PER_FOLDER:
        attachment_folder_index += 1
        current_attachment_folder = os.path.join(ATTACHMENTS_DIR, f"part_{attachment_folder_index}")
        os.makedirs(current_attachment_folder, exist_ok=True)

    return current_attachment_folder

def download_attachment(issue_key, attachment_id, filename):
    """Download an attachment from Jira using the attachment API and save it."""
    try:
        # Construct the API URL to get attachment metadata for the specific attachment
        metadata_url = f"{JIRA_BASE_URL}/rest/api/2/issue/{issue_key}/attachments/{attachment_id}"

        # Make a GET request to fetch attachment metadata
        response = requests.get(metadata_url, auth=AUTH, headers={"Accept": "application/json"}, verify=False)
        response.raise_for_status()

        # Extract the 'content' URL which contains the actual file URL
        attachment_metadata = response.json()
        file_url = attachment_metadata["content"]

        # Download the attachment using the 'content' URL
        headers = {
            "Accept": "application/octet-stream",  # Force raw file download
            "X-Atlassian-Token": "no-check"
        }

        file_response = requests.get(file_url, auth=AUTH, headers=headers, stream=True, verify=False)
        file_response.raise_for_status()

        # Save the attachment
        folder = get_attachment_folder()  # Ensure the folder path is set correctly
        filepath = os.path.join(folder, filename)

        with open(filepath, "wb") as f:
            for chunk in file_response.iter_content(chunk_size=8192):
                f.write(chunk)

        print(f"✅ Downloaded: {filename}")
        return filepath
    except requests.RequestException as e:
        print(f"❌ Failed to download {filename}: {e}")
        return None

def save_progress(last_index):
    """Save export progress for resuming."""
    with open(CHECKPOINT_FILE, "w") as f:
        json.dump({"last_index": last_index}, f)

def load_progress():
    """Load the last export progress point."""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r") as f:
            return json.load(f).get("last_index", 0)
    return 0

def process_issue(issue):
    """Process an individual issue (fetch comments & attachments)."""
    issue_key = issue["key"]
    summary = issue["fields"].get("summary", "No Summary")
    description = issue["fields"].get("description", "No Description")

    # Process issue attachments
    attachments = []
    for attachment in issue["fields"].get("attachment", []):
        attachment_id = attachment["id"]
        filename = f"{issue_key}_{attachment['filename']}"
        file_path = download_attachment(issue_key, attachment_id, filename)
        if file_path:
            attachments.append({"filename": attachment["filename"], "path": file_path})

    # Fetch comments and check for attachments
    comments = fetch_comments(issue_key)

    return {
        "issue_key": issue_key,
        "summary": summary,
        "description": description,
        "attachments": attachments,
        "comments": comments
    }

def worker(queue, output_file):
    """Thread worker function to process issues from queue."""
    while not queue.empty():
        issue = queue.get()
        processed_issue = process_issue(issue)
        with open(output_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(processed_issue) + "\n")
        queue.task_done()

def process_issues():
    """Fetch, process, and store issues with comments and attachments using multi-threading."""
    last_index = load_progress()
    part = (last_index // 50000) + 1

    while True:
        issues = fetch_issues(last_index)
        if not issues:
            print("No more issues to process.")
            break

        output_file = os.path.join(OUTPUT_DIR, f"issues_part_{part}.jsonl")
        queue = Queue()

        for issue in issues:
            queue.put(issue)

        threads = []
        for _ in range(NUM_THREADS):
            t = threading.Thread(target=worker, args=(queue, output_file))
            t.start()
            threads.append(t)

        for t in threads:
            t.join()

        last_index += len(issues)
        save_progress(last_index)

        # Rotate file every 50,000 issues
        if last_index % 50000 == 0:
            part += 1

    print(f"Export completed! Issues saved in {OUTPUT_DIR}")

if __name__ == "__main__":
    process_issues()
